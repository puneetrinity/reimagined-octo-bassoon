#!/bin/bash
# Model initialization script for RunPod A5000
# Downloads and prepares recruitment-optimized models

set -e

echo "ğŸ¤– Initializing AI models for recruitment workflow..."

# Wait for Ollama to be fully ready
echo "â³ Ensuring Ollama is ready..."
for i in {1..30}; do
    if curl -f http://localhost:11434/api/version > /dev/null 2>&1; then
        echo "âœ… Ollama is ready"
        break
    fi
    echo "Waiting for Ollama... ($i/30)"
    sleep 5
done

# Function to pull model with retry logic
pull_model() {
    local model=$1
    local max_retries=3
    local retry=1
    
    while [ $retry -le $max_retries ]; do
        echo "ğŸ“¥ Pulling $model (attempt $retry/$max_retries)..."
        if ollama pull $model; then
            echo "âœ… Successfully pulled $model"
            return 0
        else
            echo "âŒ Failed to pull $model (attempt $retry/$max_retries)"
            retry=$((retry + 1))
            sleep 10
        fi
    done
    
    echo "âš ï¸ Failed to pull $model after $max_retries attempts"
    return 1
}

# Pull recruitment-optimized models based on A5000 memory strategy
echo "ğŸ“‹ Pulling recruitment models..."

# Tier 0: Always loaded (lightweight)
echo "ğŸ”„ Tier 0: Lightweight models (always loaded)"
pull_model "phi3:mini" || echo "âš ï¸ Warning: Failed to pull phi3:mini"

# Tier 1: Keep warm (primary recruitment models)
echo "ğŸ”„ Tier 1: Primary recruitment models (keep warm)"
pull_model "deepseek-llm:7b" || echo "âš ï¸ Warning: Failed to pull deepseek-llm:7b"
pull_model "mistral:7b" || echo "âš ï¸ Warning: Failed to pull mistral:7b"

# Tier 2: Load on demand (complex reasoning)
echo "ğŸ”„ Tier 2: Complex reasoning models (on-demand)"
pull_model "llama3:8b" || echo "âš ï¸ Warning: Failed to pull llama3:8b"

# Alternative lightweight models as fallbacks
echo "ğŸ”„ Fallback models"
pull_model "tinyllama:latest" || echo "âš ï¸ Warning: Failed to pull tinyllama"

# List available models
echo "ğŸ“‹ Available models:"
ollama list

# Test model functionality
echo "ğŸ§ª Testing model functionality..."

test_model() {
    local model=$1
    echo "Testing $model..."
    if echo "Hello" | ollama run $model "Respond with 'OK' if you can process this." | grep -q "OK"; then
        echo "âœ… $model is working"
    else
        echo "âš ï¸ $model test unclear, but loaded"
    fi
}

# Test key models
test_model "phi3:mini"

echo "ğŸ¯ Model initialization complete!"
echo "ğŸ’¾ Memory usage:"
nvidia-smi --query-gpu=memory.used,memory.total --format=csv,noheader,nounits

echo "ğŸ“Š Available models for recruitment tasks:"
echo "- Resume Parsing: deepseek-llm:7b"
echo "- Bias Detection: mistral:7b" 
echo "- Matching Logic: llama3:8b"
echo "- Script Generation: llama3:8b"
echo "- Report Generation: phi3:mini"
echo "- Fallback: tinyllama:latest"