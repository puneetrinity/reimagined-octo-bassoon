"""
Analytics API Endpoints
ClickHouse-powered historical data analytics and reporting
"""

from datetime import datetime
from typing import Dict, List, Optional, Any

from fastapi import APIRouter, HTTPException, Query, Depends
from fastapi.responses import JSONResponse
import structlog

from app.storage.clickhouse_client import get_clickhouse_manager
from app.schemas.responses import create_success_response, create_error_response
from app.core.security import get_current_user_optional

router = APIRouter(prefix="/api/v1/analytics", tags=["Analytics"])
logger = structlog.get_logger(__name__)


@router.get("/cost/breakdown")
async def get_cost_breakdown(
    days: int = Query(30, ge=1, le=365, description="Number of days to analyze"),
    user_id: Optional[str] = Query(None, description="Filter by specific user ID"),
    category: Optional[str] = Query(None, description="Filter by cost category"),
    current_user: Optional[Dict] = Depends(get_current_user_optional)
) -> JSONResponse:
    """
    Get detailed cost breakdown and analytics from ClickHouse
    
    Returns cost analytics including:
    - Total costs by category and provider
    - Daily cost trends
    - Top users by cost
    - Cost optimization recommendations
    """
    try:
        clickhouse_manager = get_clickhouse_manager()
        if not clickhouse_manager:
            return create_error_response(
                message="ClickHouse analytics not available",
                error_code="CLICKHOUSE_UNAVAILABLE"
            )
        
        analytics_data = await clickhouse_manager.get_cost_analytics(
            days=days,
            user_id=user_id,
            category=category
        )
        
        if "error" in analytics_data:
            return create_error_response(
                message=f"Cost analytics query failed: {analytics_data['error']}",
                error_code="ANALYTICS_QUERY_FAILED"
            )
        
        # Add summary metrics
        total_cost = sum(item["total_cost"] for item in analytics_data.get("category_breakdown", []))
        total_requests = sum(item["request_count"] for item in analytics_data.get("category_breakdown", []))
        
        analytics_data.update({
            "summary": {
                "total_cost_usd": total_cost,
                "total_requests": total_requests,
                "avg_cost_per_request": total_cost / max(total_requests, 1),
                "analysis_period_days": days
            }
        })
        
        logger.info(
            "cost_analytics_retrieved",
            days=days,
            user_id=user_id,
            category=category,
            total_cost=total_cost,
            total_requests=total_requests
        )
        
        return create_success_response(
            data=analytics_data,
            message=f"Cost analytics for {days} days retrieved successfully"
        )
        
    except Exception as e:
        logger.error("cost_analytics_failed", error=str(e), days=days, user_id=user_id)
        return create_error_response(
            message="Failed to retrieve cost analytics",
            error_code="COST_ANALYTICS_ERROR"
        )


@router.get("/performance/trends")
async def get_performance_trends(
    days: int = Query(7, ge=1, le=90, description="Number of days to analyze"),
    current_user: Optional[Dict] = Depends(get_current_user_optional)
) -> JSONResponse:
    """
    Get system performance trends from ClickHouse
    
    Returns performance analytics including:
    - System resource trends (CPU, memory, cache)
    - Adaptive routing performance by arm
    - Response time trends
    - Success rate analysis
    """
    try:
        clickhouse_manager = get_clickhouse_manager()
        if not clickhouse_manager:
            return create_error_response(
                message="ClickHouse analytics not available",
                error_code="CLICKHOUSE_UNAVAILABLE"
            )
        
        performance_data = await clickhouse_manager.get_performance_analytics(days=days)
        
        if "error" in performance_data:
            return create_error_response(
                message=f"Performance analytics query failed: {performance_data['error']}",
                error_code="ANALYTICS_QUERY_FAILED"
            )
        
        # Calculate summary metrics
        adaptive_performance = performance_data.get("adaptive_performance", [])
        if adaptive_performance:
            total_requests = sum(item["request_count"] for item in adaptive_performance)
            weighted_success_rate = sum(
                item["success_rate"] * item["request_count"] 
                for item in adaptive_performance
            ) / max(total_requests, 1)
            
            weighted_avg_response_time = sum(
                item["avg_response_time"] * item["request_count"] 
                for item in adaptive_performance
            ) / max(total_requests, 1)
            
            performance_data["summary"] = {
                "total_adaptive_requests": total_requests,
                "overall_success_rate": weighted_success_rate,
                "avg_response_time_seconds": weighted_avg_response_time,
                "analysis_period_days": days
            }
        
        logger.info(
            "performance_analytics_retrieved",
            days=days,
            total_requests=total_requests if adaptive_performance else 0
        )
        
        return create_success_response(
            data=performance_data,
            message=f"Performance analytics for {days} days retrieved successfully"
        )
        
    except Exception as e:
        logger.error("performance_analytics_failed", error=str(e), days=days)
        return create_error_response(
            message="Failed to retrieve performance analytics",
            error_code="PERFORMANCE_ANALYTICS_ERROR"
        )


@router.get("/system/resource-usage")
async def get_system_resource_usage(
    hours: int = Query(24, ge=1, le=168, description="Number of hours to analyze"),
    current_user: Optional[Dict] = Depends(get_current_user_optional)
) -> JSONResponse:
    """
    Get detailed system resource usage trends
    
    Returns hourly system metrics including:
    - Memory usage trends
    - CPU utilization patterns
    - Cache hit rate analysis
    - Network I/O statistics
    """
    try:
        clickhouse_manager = get_clickhouse_manager()
        if not clickhouse_manager:
            return create_error_response(
                message="ClickHouse analytics not available", 
                error_code="CLICKHOUSE_UNAVAILABLE"
            )
        
        # Convert hours to days for the performance analytics query
        days = max(1, hours // 24)
        performance_data = await clickhouse_manager.get_performance_analytics(days=days)
        
        if "error" in performance_data:
            return create_error_response(
                message=f"System resource analytics query failed: {performance_data['error']}",
                error_code="ANALYTICS_QUERY_FAILED"
            )
        
        system_trends = performance_data.get("system_trends", [])
        
        # Filter to requested hour range and calculate statistics
        filtered_trends = system_trends[:hours] if len(system_trends) > hours else system_trends
        
        if filtered_trends:
            avg_memory = sum(item["avg_memory_mb"] for item in filtered_trends) / len(filtered_trends)
            avg_cpu = sum(item["avg_cpu_utilization"] for item in filtered_trends) / len(filtered_trends)
            avg_cache_hit_rate = sum(item["avg_cache_hit_rate"] for item in filtered_trends) / len(filtered_trends)
            
            peak_memory = max(item["avg_memory_mb"] for item in filtered_trends)
            peak_cpu = max(item["avg_cpu_utilization"] for item in filtered_trends)
            
            resource_analysis = {
                "time_period_hours": hours,
                "resource_trends": filtered_trends,
                "summary_statistics": {
                    "avg_memory_mb": avg_memory,
                    "avg_cpu_utilization": avg_cpu,
                    "avg_cache_hit_rate": avg_cache_hit_rate,
                    "peak_memory_mb": peak_memory,
                    "peak_cpu_utilization": peak_cpu,
                    "data_points": len(filtered_trends)
                }
            }
        else:
            resource_analysis = {
                "time_period_hours": hours,
                "resource_trends": [],
                "summary_statistics": {
                    "message": "No data available for the requested time period"
                }
            }
        
        logger.info(
            "system_resource_analytics_retrieved",
            hours=hours,
            data_points=len(filtered_trends)
        )
        
        return create_success_response(
            data=resource_analysis,
            message=f"System resource usage for {hours} hours retrieved successfully"
        )
        
    except Exception as e:
        logger.error("system_resource_analytics_failed", error=str(e), hours=hours)
        return create_error_response(
            message="Failed to retrieve system resource analytics",
            error_code="SYSTEM_RESOURCE_ANALYTICS_ERROR"
        )


@router.get("/adaptive/routing-performance")
async def get_adaptive_routing_performance(
    days: int = Query(7, ge=1, le=30, description="Number of days to analyze"),
    routing_arm: Optional[str] = Query(None, description="Filter by specific routing arm"),
    current_user: Optional[Dict] = Depends(get_current_user_optional)
) -> JSONResponse:
    """
    Get detailed adaptive routing performance analytics
    
    Returns adaptive routing metrics including:
    - Performance by routing arm
    - Success rates and response times
    - Cost efficiency analysis
    - Bandit learning progression
    """
    try:
        clickhouse_manager = get_clickhouse_manager()
        if not clickhouse_manager:
            return create_error_response(
                message="ClickHouse analytics not available",
                error_code="CLICKHOUSE_UNAVAILABLE"
            )
        
        performance_data = await clickhouse_manager.get_performance_analytics(days=days)
        
        if "error" in performance_data:
            return create_error_response(
                message=f"Adaptive routing analytics query failed: {performance_data['error']}",
                error_code="ANALYTICS_QUERY_FAILED"
            )
        
        adaptive_performance = performance_data.get("adaptive_performance", [])
        
        # Filter by routing arm if specified
        if routing_arm:
            adaptive_performance = [
                item for item in adaptive_performance 
                if item["routing_arm"] == routing_arm
            ]
        
        # Calculate routing efficiency metrics
        if adaptive_performance:
            total_requests = sum(item["request_count"] for item in adaptive_performance)
            
            # Cost efficiency ranking
            cost_ranked = sorted(adaptive_performance, key=lambda x: x["avg_cost"])
            performance_ranked = sorted(adaptive_performance, key=lambda x: x["avg_response_time"])
            success_ranked = sorted(adaptive_performance, key=lambda x: x["success_rate"], reverse=True)
            
            routing_analysis = {
                "analysis_period_days": days,
                "routing_arm_filter": routing_arm,
                "total_requests": total_requests,
                "routing_performance": adaptive_performance,
                "efficiency_rankings": {
                    "most_cost_efficient": cost_ranked[0]["routing_arm"] if cost_ranked else None,
                    "fastest_response": performance_ranked[0]["routing_arm"] if performance_ranked else None,
                    "highest_success_rate": success_ranked[0]["routing_arm"] if success_ranked else None
                },
                "recommendations": []
            }
            
            # Generate recommendations
            if len(adaptive_performance) > 1:
                best_cost = cost_ranked[0]
                worst_cost = cost_ranked[-1]
                cost_diff = worst_cost["avg_cost"] - best_cost["avg_cost"]
                
                if cost_diff > 0.01:  # $0.01 difference
                    routing_analysis["recommendations"].append(
                        f"Consider routing more traffic to '{best_cost['routing_arm']}' for cost savings"
                    )
                
                best_performance = performance_ranked[0]
                if best_performance["avg_response_time"] < 1.0:  # Sub-second response
                    routing_analysis["recommendations"].append(
                        f"'{best_performance['routing_arm']}' provides excellent response time"
                    )
        else:
            routing_analysis = {
                "analysis_period_days": days,
                "routing_arm_filter": routing_arm,
                "message": "No adaptive routing data available for the specified criteria"
            }
        
        logger.info(
            "adaptive_routing_analytics_retrieved",
            days=days,
            routing_arm=routing_arm,
            total_requests=total_requests if adaptive_performance else 0
        )
        
        return create_success_response(
            data=routing_analysis,
            message=f"Adaptive routing analytics for {days} days retrieved successfully"
        )
        
    except Exception as e:
        logger.error("adaptive_routing_analytics_failed", error=str(e), days=days, routing_arm=routing_arm)
        return create_error_response(
            message="Failed to retrieve adaptive routing analytics",
            error_code="ADAPTIVE_ROUTING_ANALYTICS_ERROR"
        )


@router.get("/dashboard/summary")
async def get_analytics_dashboard_summary(
    current_user: Optional[Dict] = Depends(get_current_user_optional)
) -> JSONResponse:
    """
    Get comprehensive analytics dashboard summary
    
    Returns high-level analytics across all data sources:
    - Cost summary (7-day and 30-day)
    - Performance summary (24-hour and 7-day)
    - System health indicators
    - Key performance indicators (KPIs)
    """
    try:
        clickhouse_manager = get_clickhouse_manager()
        if not clickhouse_manager:
            return create_error_response(
                message="ClickHouse analytics not available - showing limited data",
                error_code="CLICKHOUSE_UNAVAILABLE"
            )
        
        # Fetch multiple analytics in parallel
        import asyncio
        
        cost_7d_task = clickhouse_manager.get_cost_analytics(days=7)
        cost_30d_task = clickhouse_manager.get_cost_analytics(days=30)
        performance_1d_task = clickhouse_manager.get_performance_analytics(days=1)
        performance_7d_task = clickhouse_manager.get_performance_analytics(days=7)
        
        cost_7d, cost_30d, performance_1d, performance_7d = await asyncio.gather(
            cost_7d_task, cost_30d_task, performance_1d_task, performance_7d_task,
            return_exceptions=True
        )
        
        dashboard_summary = {
            "generated_at": datetime.now().isoformat(),
            "clickhouse_status": "connected",
            "cost_analytics": {},
            "performance_analytics": {},
            "system_health": {},
            "kpis": {}
        }
        
        # Process cost analytics
        if isinstance(cost_7d, dict) and "error" not in cost_7d:
            cost_7d_total = sum(item["total_cost"] for item in cost_7d.get("category_breakdown", []))
            requests_7d = sum(item["request_count"] for item in cost_7d.get("category_breakdown", []))
            
            dashboard_summary["cost_analytics"] = {
                "cost_7d_usd": cost_7d_total,
                "requests_7d": requests_7d,
                "avg_cost_per_request_7d": cost_7d_total / max(requests_7d, 1),
                "top_cost_category_7d": cost_7d.get("category_breakdown", [{}])[0].get("category", "unknown") if cost_7d.get("category_breakdown") else None
            }
        
        if isinstance(cost_30d, dict) and "error" not in cost_30d:
            cost_30d_total = sum(item["total_cost"] for item in cost_30d.get("category_breakdown", []))
            dashboard_summary["cost_analytics"]["cost_30d_usd"] = cost_30d_total
            
            # Calculate daily trend
            daily_trends = cost_30d.get("daily_trends", [])
            if len(daily_trends) >= 2:
                recent_avg = sum(item["daily_cost"] for item in daily_trends[:7]) / 7
                older_avg = sum(item["daily_cost"] for item in daily_trends[7:14]) / 7
                trend = "increasing" if recent_avg > older_avg else "decreasing"
                dashboard_summary["cost_analytics"]["cost_trend"] = trend
        
        # Process performance analytics
        if isinstance(performance_1d, dict) and "error" not in performance_1d:
            adaptive_perf = performance_1d.get("adaptive_performance", [])
            if adaptive_perf:
                total_requests = sum(item["request_count"] for item in adaptive_perf)
                weighted_success_rate = sum(
                    item["success_rate"] * item["request_count"] for item in adaptive_perf
                ) / max(total_requests, 1)
                weighted_response_time = sum(
                    item["avg_response_time"] * item["request_count"] for item in adaptive_perf
                ) / max(total_requests, 1)
                
                dashboard_summary["performance_analytics"] = {
                    "requests_24h": total_requests,
                    "success_rate_24h": weighted_success_rate,
                    "avg_response_time_24h": weighted_response_time,
                    "best_performing_arm": max(adaptive_perf, key=lambda x: x["success_rate"])["routing_arm"] if adaptive_perf else None
                }
        
        # System health indicators
        system_trends = performance_1d.get("system_trends", []) if isinstance(performance_1d, dict) else []
        if system_trends:
            latest_metrics = system_trends[0]  # Most recent hour
            dashboard_summary["system_health"] = {
                "current_memory_mb": latest_metrics.get("avg_memory_mb", 0),
                "current_cpu_utilization": latest_metrics.get("avg_cpu_utilization", 0),
                "current_cache_hit_rate": latest_metrics.get("avg_cache_hit_rate", 0),
                "health_status": "healthy" if latest_metrics.get("avg_cpu_utilization", 0) < 0.8 else "warning"
            }
        
        # Calculate KPIs
        dashboard_summary["kpis"] = {
            "cost_efficiency_score": min(100, (1 - dashboard_summary["cost_analytics"].get("avg_cost_per_request_7d", 0.01) / 0.01) * 100) if dashboard_summary["cost_analytics"] else 0,
            "performance_score": min(100, dashboard_summary["performance_analytics"].get("success_rate_24h", 0) * 100) if dashboard_summary["performance_analytics"] else 0,
            "system_health_score": min(100, (1 - dashboard_summary["system_health"].get("current_cpu_utilization", 0)) * 100) if dashboard_summary["system_health"] else 0
        }
        
        logger.info("analytics_dashboard_summary_generated", 
                   cost_7d_available=isinstance(cost_7d, dict),
                   performance_1d_available=isinstance(performance_1d, dict))
        
        return create_success_response(
            data=dashboard_summary,
            message="Analytics dashboard summary generated successfully"
        )
        
    except Exception as e:
        logger.error("analytics_dashboard_summary_failed", error=str(e))
        return create_error_response(
            message="Failed to generate analytics dashboard summary",
            error_code="DASHBOARD_SUMMARY_ERROR"
        )


@router.get("/health")
async def get_analytics_health() -> JSONResponse:
    """Get analytics system health status"""
    try:
        clickhouse_manager = get_clickhouse_manager()
        
        health_status = {
            "clickhouse_available": clickhouse_manager is not None,
            "clickhouse_connected": clickhouse_manager.connected if clickhouse_manager else False,
            "analytics_endpoints": [
                "/cost/breakdown",
                "/performance/trends", 
                "/system/resource-usage",
                "/adaptive/routing-performance",
                "/dashboard/summary"
            ],
            "status": "healthy" if clickhouse_manager and clickhouse_manager.connected else "degraded"
        }
        
        if clickhouse_manager and clickhouse_manager.connected:
            # Test ClickHouse connection with a simple query
            try:
                test_result = await clickhouse_manager.get_cost_analytics(days=1)
                health_status["last_query_successful"] = "error" not in test_result
            except Exception:
                health_status["last_query_successful"] = False
                health_status["status"] = "degraded"
        
        return create_success_response(
            data=health_status,
            message="Analytics health status retrieved"
        )
        
    except Exception as e:
        logger.error("analytics_health_check_failed", error=str(e))
        return create_error_response(
            message="Analytics health check failed",
            error_code="ANALYTICS_HEALTH_ERROR"
        )