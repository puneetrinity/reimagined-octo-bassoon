# Direct RunPod Deployment Dockerfile
# Build this directly on RunPod for fastest deployment
FROM python:3.10-slim

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    DEBIAN_FRONTEND=noninteractive

WORKDIR /workspace

# Install system dependencies (minimal for speed)
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl wget git build-essential redis-server supervisor \
    && rm -rf /var/lib/apt/lists/*

# Install Ollama
RUN curl -fsSL https://ollama.ai/install.sh | sh

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY app ./app
COPY scripts ./scripts
COPY .env.example .env

# Create startup script
RUN cat > start.sh << 'EOF'
#!/bin/bash
set -e

echo "🚀 Starting AI Search System on RunPod..."

# Create logs directory
mkdir -p /workspace/logs

# Start Redis
redis-server --daemonize yes --bind 0.0.0.0 --port 6379

# Start Ollama in background
ollama serve &
sleep 5

# Wait for Ollama to be ready
echo "⏳ Waiting for Ollama..."
for i in {1..30}; do
    if curl -f http://localhost:11434/api/version > /dev/null 2>&1; then
        echo "✅ Ollama ready"
        break
    fi
    sleep 2
done

# Pull essential models
echo "📥 Pulling essential models..."
ollama pull phi3:mini &
ollama pull tinyllama:latest &
wait

# Start FastAPI application
echo "🎯 Starting AI Search API..."
python -m uvicorn app.main:app --host 0.0.0.0 --port 8000
EOF

RUN chmod +x start.sh

# Create model pull script
RUN cat > pull-models.sh << 'EOF'
#!/bin/bash
echo "📥 Pulling all recruitment models..."
ollama pull phi3:mini
ollama pull tinyllama:latest  
ollama pull deepseek-llm:7b
ollama pull mistral:7b
ollama pull llama3:8b
echo "✅ All models pulled"
ollama list
EOF

RUN chmod +x pull-models.sh

EXPOSE 8000 11434 6379

HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/health/live || exit 1

CMD ["./start.sh"]